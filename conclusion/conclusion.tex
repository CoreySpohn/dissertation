Observing time for current and future space telescopes is both incredibly
valuable and a finite resource. Gathering the light required for a full
atmospheric detection of a habitable planet around a Sun-like can take weeks of
integration time \citep{TheLUVOIRTeam2019}. The work in this dissertation
investigated the efficient use of observation time for a future mission by
utilizing prior knowledge of target exoplanets. Better usage of observation
time will improve the science yield of future missions that aim to incorporate
direct imaging.

To do that I started in \Cref{cha:first_paper} by developing a method of
creating orbits consistent with radial velocity data that could be used to
schedule observations with a direct imaging instrument. This was done by
creating synthetic radial velocity data and fitting it with modern radial
velocity orbit fitting tools. Then the fitted orbital parameters were used to
create orbits with all the parameters necessary for direct imaging, propagated
in time, and compared to the true detectability of the planet used to generate
the synthetic radial velocity data.

Then in \Cref{cha:coupling} I described a method of calculating the dimmest
planet that can be detected for a specific observing scenario in the case when
there is no analytical solution possible. The problem is formulated as a
numerical minimization problem on the exposure time calculator for a specific
combination of telescope, star, and integration time. This is useful as it can
be used to calculate the probability of a specific planet type being detected
around a specific star. I applied that method to observing scenarios for the
Roman Space Telescope and determined the exoplanets that are most likely to be
detectable. With that method created, in \Cref{cha:accurate_pdet} I improved
the work described in \Cref{cha:first_paper} to calculate the probability of
detection by accounting for different integration times and variations in the
local zodiacal light and exozodiacal light.

The dissertation ends by validating of the probability of detection metric in
\Cref{cha:sim_and_scheduling}. I created a framework for simulating the full
process of collecting and using radial velocity data for direct imaging.
Realistic radial velocity observation runs are performed and blind fitting is
done to find planets from the synthetic radial velocity data. Then the fitted
orbits are used to calculate the probability of detecting the planets in the
simulation. A constraint programming method was used to create an observation
schedule in line with the current goals of a direct imaging mission. By running
full simulations I showed that the method is able to consistently schedule
successful observations of Earth-like exoplanets when they have prior radial
velocity data.

This work proves that it is possible to use the incomplete orbital information
that a radial velocity orbit fit provides to inform a future direct imaging
mission that will search for habitable planets outside of our solar system. The
tools I created also allow for studies on a number of interesting questions
such as observing strategies for radial velocity instruments, systematic
evaluation of different orbit fitting tools, and the impact of reaching
different radial velocity precisions on the number of Earth-like atmospheres a
future direct imaging mission will be able characterize.

There are a number of interesting open questions that the work in this
dissertation is related to. In creating the scheduler in
\Cref{cha:sim_and_scheduling} there was an assumption that observations should
only be made when the planet is detectable. However, it is reasonable to
believe that there are cases when an observation that does not detect a planet
provides more orbital information by ruling out all orbits that would be
detectable at the observation time. Doing this would involve calculating some
measure of entropy in the fitted orbits, which was applied to RV observation
scheduling in \citet{loredoBayesianMethodsAnalysis2012}.
\citet{loredoBayesianMethodsAnalysis2012} calculated entropy based on the
Kullback-Leibler divergence of predicted RV values and identified maximum
entropy sampling as the best method to reduce uncertainty in the RV orbital
fit. A similar method could be adapted for direct imaging where the divergence
of the orbital fit's predicted position in the image frame is used as the
measure of entropy.

Another extension of this process would be to create a scheduler that is
reactive to failed observations. The scheduler in \Cref{cha:sim_and_scheduling}
only works uses a single $n_\textrm{EZ}$ value for all probability of detection
calculations even though a failed observation is likely due to observing a
planetary system with a higher $n_\textrm{EZ}$ value. A way to handle this
could be to recalculate the probability of detection values with a higher
$n_\textrm{EZ}$ value after a failed observation, then recreate the schedule.
It would be computationally costly to recalculate the full schedule but it
would not be difficult to hold all currently scheduled observations constant
and reschedule observations for just the star that had the failed observation.

A fully dynamic direct imaging mission with infinite computational resources
would want to run orbit fitting after every observation, recalculate metrics
relevant to observation scheduling (probability of detection or entropy), and
fully recreate its observation schedule to maximize the number of detected
planets (assuming there are no competing science goals for the mission).
Running for Monte Carlo mission simulations with full fidelity is impractical,
but it could be possible to create heuristics that estimate the orbital fit
without actually running orbit fitting. Estimating orbital fits could be done
using the fact that a Monte Carlo mission simulation has access to the true
orbital parameters of a planet if there is also a method of determining what
the uncertainties on those parameters would be given the observations made
during the mission simulation. No such method currently exists but simulating
the orbit fitting process for many simulated exoplanet data sets and analyzing
the resulting orbit fits could lead to such a method.
